% ------------------------------------------------------------------------
% Artigo de Exemplo no Formato SBC - Projeto de Big Data (DS340 - UFPR)
% ------------------------------------------------------------------------

\documentclass[12pt]{article}

\usepackage{sbc-template}
\usepackage{graphicx,url}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}

\title{Análise Comparativa de Desempenho entre Databricks e PostgreSQL no Armazenamento e Consulta de Dados Relacionados a Aderência em Cursos Tecnológicos}

\author{Anna Luiza Mariano\inst{1}, Arthur Deretti\inst{1}, Maria Eduarda Ferreira\inst{1}, Matheus Xavier\inst{1}, Rafael Pompônio\inst{1}}

\address{Curso de Tecnologia em Análise e Desenvolvimento de Sistemas \\ Universidade Federal do Paraná (UFPR)\\
}

\begin{document}

\maketitle

\begin{resumo}
Este artigo apresenta uma análise comparativa entre a plataforma de processamento de dados Databricks e o sistema relacional PostgreSQL, aplicados ao armazenamento e consulta de dados relacionados a aderência em cursos tecnológicos obtidos a partir do portal público do SISU (Sistema de Seleção Unificada). O objetivo é avaliar o desempenho de cada sistema em operações de leitura, atualização e agregação de dados em um cenário de Big Data.
\end{resumo}

\section{Introdução}

O crescimento exponencial do volume de dados gerados por sensores, redes sociais e dispositivos conectados trouxe novos desafios para o armazenamento e a manipulação eficiente de informações. Nesse contexto, surgiram as tecnologias de \textit{Big Data}, que oferecem soluções escaláveis para o tratamento de grandes volumes de dados de maneira distribuída.

Este trabalho tem como objetivo comparar o desempenho de uma tecnologia BigData, o Databricks, com o sistema relacional PostgreSQL, aplicando ambas as ferramentas a um conjunto de dados institucionais reais. A análise foca no tempo de execução, uso de memória e facilidade de modelagem.

\section{Tecnologia Big Data Utilizada}

O \textbf{Databricks} é uma plataforma analítica aberta e unificada para criação, implantação, compartilhamento e manutenção de análises de dados de nível empresarial, e utiliza tecnologias que garantem desempenho, escalabilidade e controle transacional sobre grandes volumes de dados.

A modelagem de dados no Databricks baseia-se na arquitetura Data Lakehouse, que combina a flexibilidade dos Data Lakes com a governança dos Data Warehouses. Os dados podem ser estruturados, semiestruturados ou não estruturados, e são organizados em camadas: bronze (dados brutos), silver (dados tratados) e gold (dados prontos para análise). O processamento ocorre de forma distribuída, permitindo tanto cargas em lote (batch) quanto em tempo real (streaming).

No mercado, o Databricks é amplamente utilizado por empresas que trabalham com Big Data e inteligência artificial. Organizações utilizam a plataforma para análise preditiva, detecção de fraudes, personalização de serviços e otimização de processos industriais. Essa adoção se deve à capacidade da plataforma de lidar com grandes volumes de dados de forma eficiente e colaborativa.

Em comparação com bancos de dados relacionais, o Databricks se destaca em cenários que exigem escalabilidade, processamento paralelo e integração com dados não estruturados. Enquanto os bancos relacionais são mais adequados para aplicações transacionais e dados estruturados, o Databricks é ideal para análise de grandes volumes de informações, aprendizado de máquina e processamento em tempo real, características típicas de ambientes de Big Data.

\section{Estudo de Caso}

O estudo de caso utiliza dados públicos fornecidos pelo \textit{Sistema de Seleção Unificada (Sisu)}. O conjunto contém registros de modalidade de concorrência, sexo e cpf dos concorrentes, instituição de ensino e nome do curso tecnológico, totalizando aproximadamente 400 mil linhas.

Foram realizadas operações de importação, filtragem por , cálculo de médias e agregações por período. O mesmo conjunto foi carregado e consultado tanto no Databricks quanto no PostgreSQL, utilizando scripts Python para execução automatizada e coleta de métricas.

\section{Resultados e Avaliações}

Os testes foram realizados em uma máquina com processador Intel i7, 16 GB de RAM e SSD de 512 GB, rodando Ubuntu 22.04. Cada operação foi executada três vezes, e o tempo médio foi registrado.

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Operação} & \textbf{PostgreSQL (ms)} & \textbf{MongoDB (ms)} \\ \hline
Inserção de Dados & 261 & 731 \\ \hline
Atualização        & 3   & 7   \\ \hline
Consulta Média     & 47  & 2   \\ \hline
Busca de Texto     & 124 & 39  \\ \hline
\end{tabular}
\end{center}

Os resultados mostram que o MongoDB teve desempenho superior em consultas simples e buscas textuais, enquanto o PostgreSQL se destacou em operações de escrita e consistência transacional. Em termos de uso de memória, o MongoDB apresentou consumo cerca de 30\% maior, mas com menor tempo de resposta.

\section{Conclusão}

Com base nos experimentos realizados, conclui-se que o MongoDB oferece maior flexibilidade e velocidade em consultas com alto volume de dados sem estrutura rígida, sendo ideal para aplicações que priorizam escalabilidade. Já o PostgreSQL se mantém mais adequado em contextos que exigem integridade relacional e consistência de dados.

Recomenda-se o uso do MongoDB em cenários de coleta e análise contínua de dados, como sistemas meteorológicos e de IoT, enquanto o PostgreSQL é mais indicado para aplicações financeiras e corporativas.


\begin{thebibliography}{99}

\bibitem{databricks}
Introdução ao Databricks no AWS. Disponível em: \url{https://docs.databricks.com/aws/pt/introduction/#casos-de-uso-comuns}
. Acesso em: nov. 2025.

\bibitem{mongodb}
Chodorow, K. (2013). \textit{MongoDB: The Definitive Guide}. O'Reilly Media.

\bibitem{postgresql}
Momjian, B. (2021). \textit{PostgreSQL: Introduction and Concepts}. Addison-Wesley.

\bibitem{sbc}
Sociedade Brasileira de Computação. \textit{Template para Artigos e Capítulos de Livros}. Disponível em: \url{http://www.sbc.org.br/documentos-da-sbc}.

\end{thebibliography}

\section*{Anexos}

Os scripts utilizados para importação e consulta dos dados foram desenvolvidos em Python, utilizando as bibliotecas \textit{pymongo} e \textit{psycopg2}. O código completo, incluindo o \textit{how-to} de execução, encontra-se disponível em repositório GitHub vinculado ao projeto.

\end{document}